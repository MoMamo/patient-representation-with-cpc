{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.insert(0, '/home/yikuan/project/git/CPRD')\n",
    "\n",
    "from utils.yaml_act import yaml_load\n",
    "from utils.arg_parse import arg_paser\n",
    "from CPRD.config.spark import spark_init, read_parquet, read_txt\n",
    "import pyspark.sql.functions as F\n",
    "from CPRD.functions import tables, merge, cohort_select,risk_prediction, modalities, MedicalDictionary, risk_prediction, predictor_extractor\n",
    "from CPRD.functions import merge\n",
    "from utils.utils import save_obj, load_obj\n",
    "import pandas as pd\n",
    "from pyspark.sql import Window\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from CPRD.config.utils import cvt_str2time\n",
    "from CPRD.config.utils import check_time\n",
    "from CPRD.config.utils import RangeExtract\n",
    "\n",
    "class dotdict(dict):\n",
    "    \"\"\"dot.notation access to dictionary attributes\"\"\"\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yikuan/project/git/CPRD/utils/yaml_act.py:6: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  cfg = yaml.load(ymlfile)\n"
     ]
    }
   ],
   "source": [
    "args = dotdict({'params': '/home/yikuan/project/git/CPRD/config/config.yaml'})\n",
    "params = yaml_load(args.params)\n",
    "spark_params = params['pyspark']\n",
    "spark = spark_init(spark_params)\n",
    "file = params['file_path']\n",
    "# data_params = params['params']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the cohort file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demographics = read_parquet(spark.sqlContext, '/home/shared/yikuan/HF_Valid/data/demographics.parquet/').drop(\"study_entry\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocess diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process diagnoses for exclusion criteria\n",
    "diagnoses = read_parquet(spark.sqlContext, '/home/shared/yikuan/HF_Valid/data/diagnosis.parquet')\n",
    "diag_cprd = diagnoses.filter(F.col('source')=='CPRD').select(['patid', 'eventdate', 'medcode']).withColumnRenamed('medcode', 'code')\n",
    "diag_hes = diagnoses.filter(F.col('source')=='HES').select(['patid', 'eventdate', 'ICD']).withColumnRenamed('ICD', 'code')\n",
    "diagnoses = diag_cprd.union(diag_hes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "medications = read_parquet(spark.sqlContext, '/home/shared/yikuan/HF_Valid/data/medications.parquet').select(['patid', 'prodcode', 'eventdate', 'bnfcode', 'code'])\n",
    "medications = medications.withColumn('bnf6', F.col('bnfcode').substr(1, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find patients who have hear failure in the records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_query = MedicalDictionary.MedicalDictionaryRiskPrediction(file, spark)\n",
    "\n",
    "hf = condition_query.queryDisease(['Heart failure'], merge=True)\n",
    "hf_code = {'hf': []}\n",
    "for k,v in hf['heart failure'].items():\n",
    "    hf_code['hf'].extend(v)\n",
    "condition = hf_code['hf']\n",
    "\n",
    "hf = diagnoses.filter(F.col('code').isin(*condition))\n",
    "w = Window.partitionBy('patid').orderBy('eventdate')\n",
    "hf = hf.withColumn('code', F.first('code').over(w)).groupBy('patid').agg(\n",
    "    F.min('eventdate').alias('eventdate'),\n",
    "    F.first('code').alias('code')\n",
    ")\n",
    "\n",
    "demographics = demographics.join(hf, 'patid', 'left').dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selection criteria\n",
    "1. 2 year before the incident HF\n",
    "2. at least 2 years registration with GP at the baseline\n",
    "3. at least 35 years old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demographics = demographics.withColumn('baseline', demographics.eventdate - F.expr('INTERVAL 2 YEARS'))\n",
    "demographics = demographics.filter(F.col('baseline') > F.col('start'))\n",
    "\n",
    "demographics.write.parquet('/home/shared/yikuan/Mo/demographics.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Patient predictor selection\n",
    "1. age\n",
    "2. renal function\n",
    "3. blood pressure\n",
    "4. sodium\n",
    "5. ejection fraction\n",
    "6. gender\n",
    "7. BNP\n",
    "8. NYHA class\n",
    "9. diabetes\n",
    "10. BMI\n",
    "11. excercise tolerance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort = read_parquet(spark.sqlContext, '/home/shared/yikuan/Mo/demographics.parquet').select(['patid', 'gender', 'dob', 'baseline'])\n",
    "\n",
    "def code_merge(code_dict, name):\n",
    "    code = []\n",
    "    for k,v in code_dict[name].items():\n",
    "        code.extend(v)\n",
    "    return code\n",
    "\n",
    "predictor = predictor_extractor.PredictorExtractorBase()\n",
    "\n",
    "# age calculation\n",
    "age_cal = F.unix_timestamp('baseline', \"yyyy-MM-dd\") - F.unix_timestamp('dob', \"yyyy-MM-dd\")\n",
    "cohort = cohort.withColumn('age', age_cal).withColumn('age', (F.col('age') / 3600 / 24 / 30/ 12).cast('integer'))\n",
    "\n",
    "# blood pressure\n",
    "bp = modalities.retrieve_systolic_bp_measurement(file, spark, duration=(1985, 2015), usable_range=(60, 200)).select(['patid', 'eventdate', 'systolic'])\n",
    "bp = predictor.predictor_extract(bp, cohort, 'systolic', col_baseline='baseline', span_before_baseline_month=24, type='mean')\n",
    "cohort = cohort.join(bp, 'patid', 'left')\n",
    "\n",
    "# sodium\n",
    "sodium = tables.retrieve_additional(dir=file['test'], spark=spark).filter(F.col('enttype') == 196)\n",
    "sodium = RangeExtract(sodium, 'data2', (70, 200))\n",
    "sodium = sodium.withColumn('eventdate', cvt_str2time(sodium, 'eventdate', year_first=True))\n",
    "sodium = check_time(sodium, 'eventdate', time_a=1985, time_b=2015).select(['patid','eventdate','data2']).withColumnRenamed('data2', 'sodium').withColumn('sodium', F.col('sodium').astype(\"float\"))\n",
    "sodium = predictor.predictor_extract(sodium, cohort, 'sodium', col_baseline='baseline', span_before_baseline_month=24,type='mean')\n",
    "cohort = cohort.join(sodium, 'patid', 'left')\n",
    "\n",
    "# diabetes\n",
    "diabetes = condition_query.queryDisease(['Diabetes'], merge=True)\n",
    "diabetes = code_merge(diabetes, 'diabetes')\n",
    "diabetes = predictor.predictor_check_exist(diabetes, diagnoses, cohort, col='code', col_baseline='baseline').withColumnRenamed('code', 'diabetes')\n",
    "cohort = cohort.join(diabetes, 'patid', 'left')\n",
    "\n",
    "cohort.write.parquet('/home/shared/yikuan/Mo/predictor.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort = read_parquet(spark.sqlContext, '/home/shared/yikuan/Mo/predictor.parquet')\n",
    "\n",
    "# bmi\n",
    "bmi = modalities.retrieve_bmi(file, spark, duration=(1985, 2015), usable_range=(16, 50))\n",
    "bmi = predictor.predictor_extract(bmi, cohort, 'BMI', col_baseline='baseline', span_before_baseline_month=24, type='mean')\n",
    "cohort = cohort.join(bmi, 'patid', 'left')\n",
    "\n",
    "# hdl ratio\n",
    "hdl_r = tables.retrieve_additional(dir=file['test'], spark=spark).filter(F.col('enttype') == 338)\n",
    "hdl_r = RangeExtract(hdl_r, 'data2', (0, 50))\n",
    "\n",
    "hdl_r = hdl_r.withColumn('eventdate', cvt_str2time(hdl_r, 'eventdate', year_first=True))\n",
    "hdl_r = check_time(hdl_r, 'eventdate', time_a=1985, time_b=2015).select(['patid','eventdate','data2']).withColumnRenamed('data2', 'hdl_r').withColumn('hdl_r', F.col('hdl_r').astype(\"float\"))\n",
    "hdl_r = predictor.predictor_extract(hdl_r, cohort, 'hdl_r', col_baseline='baseline', span_before_baseline_month=24,type='mean')\n",
    "cohort = cohort.join(hdl_r, 'patid', 'left')\n",
    "\n",
    "# Chronic kidney disease (stage 4 or 5) and major chronic renal disease (including nephrotic syndrome, chronic glomerulonephritis, chronic pyelonephritis, renal dialysis, and renal transplant)\n",
    "ckd = ['T86', 'T861', 'N18', 'N189', 'N18', 'N185', 'N18', 'N184', 'N18', 'N183', 'N18', 'N182', 'N18', 'N181', 'N11', 'N110', 'N07', 'N074', 'N07', 'N073', 'N07', 'N072', 'N03', 'N030', 'N00', 'N000', '99644', '99631', '97980', '97979', '97978', '97758', '97734', '97683', '97587', '97388', '95572', '95571', '95546', '95508', '95408', '95406', '95405', '95188', '95180', '95179', '95178', '95177', '95176', '95175', '95146', '95145', '95123', '95122', '95121', '94965', '94793', '94789', '94373', '94350', '93922', '91738', '89924', '88597', '85991', '73026', '72877', '72303', '72004', '71709', '71314', '71174', '70874', '68659', '68364', '68114', '68112', '67995', '67486', '67232', '67197', '67193', '66872', '66714', '66705', '66505', '66062', '65400', '65064', '64828', '64622', '64571', '63786', '63615', '63466', '63000', '62980', '62520', '62320', '61811', '61494', '61344', '61317', '61145', '60960', '60857', '60856', '60796', '60484', '60198', '60128', '59365', '59031', '59018', '58750', '58671', '58164', '58060', '57621', '57568', '57278', '57168', '57072', '56987', '56893', '56852', '55151', '54990', '53940', '53852', '52969', '52303', '51113', '50728', '50472', '50331', '50305', '50225', '50200', '49642', '48855', '48111', '47922', '47672', '47582', '47342', '46963', '46438', '46145', '45867', '45499', '45160', '44270', '43935', '41881', '41676', '41285', '41239', '41148', '41013', '40413', '40349', '39840', '39649', '38572', '36342', '36205', '36125', '35360', '35107', '35105', '34998', '34648', '34637', '32423', '30323', '30301', '30294', '29638', '29013', '28684', '26054', '25394', '25055', '24836', '24384', '24361', '23913', '22852', '22252', '22205', '21989', '21983', '21947', '21837', '21297', '21158', '20629', '20516', '20073', '19316', '18777', '18774', '18390', '18209', '17365', '16929', '16502', '16008', '15917', '15106', '15097', '13279', '12720', '12640', '12586', '12585', '12566', '12479', '12465', '11875', '11773', '11745', '11553', '10809', '10647', '10418', '9840', '9240', '8330', '8037', '7804', '7190', '6712', '5911', '5504', '4669', '4668', '4654', '4503', '2997', '2996', '2475', '2471', '1803', '512']\n",
    "ckd = predictor.predictor_check_exist(ckd, diagnoses, cohort, col='code', col_baseline='baseline').withColumnRenamed('code', 'ckd')\n",
    "cohort = cohort.join(ckd, 'patid', 'left')\n",
    "\n",
    "# Family history of coronary heart disease in a first degree relative aged less than 60 years\n",
    "history = modalities.retrieve_by_enttype(file, spark, 87, (1985, 2015)).withColumnRenamed('data1', 'history').select(['patid', 'history'])\n",
    "chd = condition_query.queryDisease(['coronary heart disease not otherwise specified'], merge=True)\n",
    "chd = code_merge(chd, 'coronary heart disease not otherwise specified')\n",
    "history = history.filter(F.col('history').isin(*chd)).groupby(['patid']).agg(F.first('history').alias('chd_history'))\n",
    "cohort = cohort.join(history, 'patid', 'left')\n",
    "\n",
    "cohort.write.parquet('/home/shared/yikuan/Mo/predictor_1.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort = read_parquet(spark.sqlContext, '/home/shared/yikuan/Mo/predictor_1.parquet')\n",
    "\n",
    "# Treated hypertension (diagnosis of hypertension and treatment with at least one antihypertensive drug)\n",
    "antihyp = condition_query.queryMedication(['antihypertensives'])\n",
    "antihyp = antihyp.get('antihypertensives').get('prod')\n",
    "antihyp = predictor.predictor_check_exist(antihyp, medications, cohort, col='prodcode', col_baseline='baseline').withColumnRenamed('prodcode', 'antihtn')\n",
    "cohort = cohort.join(antihyp, 'patid', 'left')\n",
    "\n",
    "# smoking status\n",
    "## smoking status 0: not recorded 1: non smoker 2: ex 3: light (1-9) 4:moderate (10-19) 5: heavy (>=20) \n",
    "def categorise_smoke(x):\n",
    "    if int(x)>=1 and int(x)<=9:\n",
    "        return 3\n",
    "    elif int(x)>=10 and int(x)<=19:\n",
    "        return 4\n",
    "    elif int(x)>=20:\n",
    "        return 5\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def smoke_dict(x):\n",
    "    map_dict = {\n",
    "        '0': 0,\n",
    "        '2': 1,\n",
    "        '3': 2\n",
    "    }\n",
    "    return map_dict[str(x)]\n",
    "\n",
    "smoke_cat = F.udf(lambda x: categorise_smoke(x))\n",
    "smoke_map = F.udf(lambda x: smoke_dict(x))\n",
    "\n",
    "smoke = modalities.retrieve_smoking_status(file, spark, duration=(1985, 2015))\n",
    "smoke_current = smoke.filter(F.col('smoke')==1).filter(F.col('cig_per_day')!='').withColumn('cig_per_day', smoke_cat('cig_per_day')).drop('smoke')\n",
    "smoke_current = smoke_current.withColumnRenamed('cig_per_day', 'smoke')\n",
    "\n",
    "smoke_other = smoke.filter(F.col('smoke')!=1).withColumn('smoke', smoke_map('smoke')).select(['patid', 'eventdate', 'smoke'])\n",
    "smoke = smoke_current.union(smoke_other)\n",
    "smoke = smoke.filter(F.col('smoke')!=0)\n",
    "smoke = predictor.predictor_extract(smoke, cohort, 'smoke', col_baseline='baseline', span_before_baseline_month=24,type='last')\n",
    "cohort = cohort.join(smoke, 'patid', 'left')\n",
    "\n",
    "# Deprivation\n",
    "deprivation = modalities.retrieve_imd(file, spark)\n",
    "cohort = cohort.join(deprivation, 'patid', 'left')\n",
    "\n",
    "# AF\n",
    "AF = condition_query.queryDisease(['Atrial fibrillation'], merge=True)\n",
    "AF = code_merge(AF, 'atrial fibrillation')\n",
    "AF = predictor.predictor_check_exist(AF, diagnoses, cohort, col='code', col_baseline='baseline').withColumnRenamed('code', 'atrial_fibrillation')\n",
    "cohort = cohort.join(AF, 'patid', 'left')\n",
    "\n",
    "cohort.write.parquet('/home/shared/yikuan/Mo/predictor_2.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clinical outcome interested in \n",
    "1. mortality (5-year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort = read_parquet(spark.sqlContext, '/home/shared/yikuan/Mo/demographics.parquet/')\n",
    "death = tables.retrieve_death(dir=file['death'], spark=spark).select(['patid', 'dod'])\n",
    "cohort = cohort.join(death, 'patid', 'left')\n",
    "\n",
    "# set up the 5 year date theshold for each patient (baseline + 7 years)\n",
    "cohort = cohort.withColumn('endfollowupdate', cohort.baseline + F.expr('INTERVAL 7 YEARS')).cache()\n",
    "\n",
    "# do filtering\n",
    "cohort_no_event = cohort.filter(F.col('dod').isNull()).withColumn('event', F.lit(0)).withColumn('time', F.least(F.col('enddate'), F.col('endfollowupdate')))\n",
    "cohort_with_event = cohort.filter(F.col('dod').isNotNull()).filter(F.col('dod') > F.col('baseline')).cache()\n",
    "\n",
    "cohort_with_event_a = cohort_with_event.filter(F.col('dod') > F.col('endfollowupdate')).withColumn('event', F.lit(0)).withColumn('time', F.col('endfollowupdate'))\n",
    "cohort_with_event_b = cohort_with_event.filter((F.col('dod') < F.col('endfollowupdate')) & (F.col('dod') > F.col('baseline'))).withColumn('event', F.lit(1)).withColumn('time', F.col('dod'))\n",
    "\n",
    "cohort = cohort_no_event.union(cohort_with_event_a).union(cohort_with_event_b).drop('endfollowupdate')\n",
    "\n",
    "time2eventdiff = F.unix_timestamp('time', \"yyyy-MM-dd\") - F.unix_timestamp('baseline', \"yyyy-MM-dd\")\n",
    "cohort = cohort.withColumn('time', time2eventdiff).withColumn('time', (F.col('time') / 3600 / 24 / 30).cast('integer'))\n",
    "cohort = cohort.filter(F.col('time')>0)\n",
    "cohort.write.parquet('/home/shared/yikuan/Mo/cohort.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data imputation\n",
    "cohort = read_parquet(spark.sqlContext, '/home/shared/yikuan/Mo/cohort.parquet')\\\n",
    "    .select(['patid', 'event', 'time'])\n",
    "\n",
    "predictor = read_parquet(spark.sqlContext, '/home/shared/yikuan/Mo/predictor.parquet')\\\n",
    "    .select(['patid', 'gender', 'age', 'systolic', 'sodium', 'diabetes', 'hdl_r', 'ckd', 'chd_history', 'antihtn',\n",
    "            'smoke', 'imd2015_5', 'atrial_fibrillation'])\n",
    "\n",
    "cohort = cohort.join(predictor, 'patid', 'left').filter(F.col('imd2015_5').isNotNull())\n",
    "cohort.write.parquet('/home/shared/yikuan/Mo/mortality.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MICE imputation\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# eth_map = {\n",
    "#     'Unknown': \"0\",\n",
    "#     'White': '1',\n",
    "#     'Oth_Asian': '2', \n",
    "#     'Pakistani': \"3\",\n",
    "#     'Indian': \"4\",\n",
    "#     'Other': \"5\",\n",
    "#     'Bl_Carib': \"6\",\n",
    "#     'Mixed': \"7\", \n",
    "#     'Bangladesi': \"8\", \n",
    "#     'Chinese': \"9\", \n",
    "#     'Bl_Other': \"10\",\n",
    "#     'Bl_Afric': \"11\"\n",
    "# }\n",
    "    \n",
    "b = pd.read_parquet('/home/shared/yikuan/Mo/mortality.parquet')\n",
    "# b['gen_ethnicity'] = b.gen_ethnicity.apply(lambda x: eth_map.get(x))\n",
    "\n",
    "feature = ['gender', 'age', 'systolic', 'sodium', 'diabetes', 'hdl_r', 'ckd', 'chd_history', 'antihtn','smoke', 'imd2015_5', 'atrial_fibrillation']\n",
    "\n",
    "rest = [each for each in b.columns if each not in feature]\n",
    "\n",
    "imp = IterativeImputer(max_iter=5, random_state=0)\n",
    "imp.fit(b[feature])\n",
    "\n",
    "imput = imp.transform(b[feature])\n",
    "\n",
    "b_rest = b[rest]\n",
    "imput = pd.DataFrame(imput, columns=feature)\n",
    "\n",
    "b_rest = b_rest.join(imput)\n",
    "\n",
    "b_rest.to_parquet('/home/shared/yikuan/Mo/mortality_imp.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BEHRT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort = read_parquet(spark.sqlContext, '/home/shared/yikuan/Mo/cohort.parquet')\n",
    "cohort = cohort.select(['patid', 'dob', 'baseline', 'event', 'time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrive medications\n",
    "medications = read_parquet(spark.sqlContext, '/home/shared/yikuan/HF_Valid/data/medications.parquet')\n",
    "medications = medications.select(['patid', 'eventdate', 'code']).withColumn('code', F.concat(F.lit('MED'), F.col('code'))).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve procedure\n",
    "procedure = read_parquet(spark.sqlContext, '/home/shared/yikuan/HF_Valid/data/procedure.parquet')\n",
    "procedure = procedure.select(['patid', 'eventdate', 'OPCS']).withColumnRenamed('OPCS', 'code').withColumn('code', F.concat(F.lit('PRO'), F.col('code'))).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_test = read_parquet(spark.sqlContext, '/home/shared/yikuan/HF_Valid/data/lab_test.parquet')\n",
    "lab_test = lab_test.select(['patid', 'eventdate', 'medcode']).withColumnRenamed('medcode', 'code').withColumn('code', F.concat(F.lit('LAB'), F.col('code'))).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve diagnosis\n",
    "diagnoses = read_parquet(spark.sqlContext, '/home/shared/yikuan/HF_Valid/data/diagnosis.parquet')\n",
    "diagnoses = diagnoses.select(['patid', 'eventdate', 'ICD']).withColumnRenamed('ICD', 'code').withColumn('code', F.concat(F.lit('DIA'), F.col('code'))).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up formater\n",
    "behrt_formater = predictor_extractor.BEHRT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = medications.union(procedure).union(lab_test).union(diagnoses)\n",
    "data = behrt_formater.format_behrt(data, cohort, col_entry='baseline', col_yob='dob', age_col_name='age', col_code='code', label=None, event='event', time='time').dropna()\n",
    "data.write.parquet('/home/shared/yikuan/Mo/BEHRT.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_parquet(spark.sqlContext, '/home/shared/yikuan/Mo/BEHRT.parquet')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check the records one year after HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "medications = read_parquet(spark.sqlContext, '/home/shared/yikuan/HF_Valid/data/medications.parquet')\n",
    "medications = medications.select(['patid', 'eventdate', 'prodcode']).withColumn('code', F.concat(F.lit('MED'), F.col('prodcode'))).dropna().drop('prodcode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cohort = read_parquet(spark.sqlContext, '/home/shared/yikuan/Mo/cohort.parquet/')\n",
    "cohort = cohort.withColumn('checkdate', cohort.eventdate + F.expr('INTERVAL 1 YEARS')).select(['patid', 'eventdate', 'checkdate']).withColumnRenamed('eventdate', 'hf_date')\n",
    "data = medications.union(procedure).union(lab_test)\n",
    "\n",
    "# keep records between event date and the check date\n",
    "data = data.join(cohort, 'patid', 'inner').dropna() \\\n",
    "    .filter((F.col('eventdate') <= F.col('checkdate')) & (F.col('eventdate') > F.col('hf_date')))\n",
    "\n",
    "# collect the codes for each patient\n",
    "data = data.groupby(['patid', 'eventdate']).agg(F.collect_list('code').alias('code'))\n",
    "\n",
    "# sort and merge code\n",
    "w = Window.partitionBy('patid').orderBy('eventdate')\n",
    "data = data.withColumn('code', F.collect_list('code').over(w)) \\\n",
    "    .groupBy('patid').agg(F.max('code').alias('code'))\n",
    "data = data.withColumn('code', F.flatten(F.col('code')))\n",
    "\n",
    "\n",
    "data.write.parquet('/home/shared/yikuan/Mo/hf_records_one_year_v1.parquet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# implement other event of interest for outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "# from CPRD.functions import tables, merge\n",
    "# import pyspark.sql.functions as F\n",
    "# from CPRD.config.utils import *\n",
    "# from CPRD.functions.modalities import *\n",
    "# from pyspark.sql import Window\n",
    "# from CPRD.config.spark import read_parquet\n",
    "# from typing import Any\n",
    "# import datetime\n",
    "# from pyspark.sql.types import IntegerType\n",
    "\n",
    "# def prepareDeathCause(death, condition, column):\n",
    "#     cause_cols = ['cause', 'cause1', 'cause2', 'cause3', 'cause4', 'cause5', 'cause6', 'cause7',\n",
    "#                   'cause8', 'cause9', 'cause10', 'cause11', 'cause12', 'cause13', 'cause14', 'cause15']\n",
    "#     cause_cols = [F.col(each) for each in cause_cols]\n",
    "#     death = death.withColumn(\"cause\", F.array(cause_cols)).select(['patid', 'cause', 'dod'])\n",
    "#     rm_dot = F.udf(lambda x: x.replace(\".\", \"\"))\n",
    "#     death = death.withColumn('cause', F.explode('cause')) \\\n",
    "#         .withColumn('cause', rm_dot('cause')) \\\n",
    "#         .filter(F.col('cause').isin(*condition))\n",
    "#     death = death.groupBy('patid').agg(F.first('dod').alias('eventdate'), F.first('cause').alias(column))\n",
    "#     return death\n",
    "\n",
    "# def process_death_diagnoses(diagnoses, death, condition, column):\n",
    "#     death = prepareDeathCause(death, condition, column)\n",
    "#     diagnoses = diagnoses.filter(F.col(column).isin(*condition))\n",
    "#     source = diagnoses.union(death)\n",
    "#     return source\n",
    "\n",
    "# def event2time(cohort, source, baseline, column, follow_up_duration, event_col, time_col):\n",
    "#     tmp = cohort.select(['patid', baseline])\n",
    "#     source = source.join(tmp, 'patid', 'left').filter(F.col('eventdate')>F.col(baseline)).dropna()\n",
    "    \n",
    "#     w = Window.partitionBy('patid').orderBy('eventdate')\n",
    "#     source = source.withColumn(column, F.first(column).over(w)).groupBy('patid').agg(\n",
    "#         F.first('eventdate').alias('eventdate'),\n",
    "#         F.first(column).alias(column)\n",
    "#     )\n",
    "    \n",
    "#     cohort = cohort.join(source, 'patid', 'left').drop(column)\n",
    "#     cohort = cohort.withColumn('endfollowupdate', cohort.baseline + F.expr('INTERVAL {} MONTHS'.format(follow_up_duration))).cache()\n",
    "\n",
    "#     cohort_no_event = cohort.filter(F.col('eventdate').isNull()).withColumn(event_col, F.lit(0)).withColumn(time_col, F.least(F.col('enddate'), F.col('endfollowupdate')))\n",
    "#     cohort_with_event = cohort.filter(F.col('eventdate').isNotNull()).cache()\n",
    "    \n",
    "#     cohort_with_event_a = cohort_with_event.filter(F.col('eventdate') > F.col('endfollowupdate')).withColumn(event_col, F.lit(0)).withColumn(time_col, F.col('endfollowupdate'))\n",
    "#     cohort_with_event_b = cohort_with_event.filter((F.col('eventdate') <= F.col('endfollowupdate'))).withColumn(event_col, F.lit(1)).withColumn(time_col, F.col('eventdate'))\n",
    "    \n",
    "#     cohort = cohort_no_event.union(cohort_with_event_a).union(cohort_with_event_b)\n",
    "\n",
    "\n",
    "#     time2eventdiff = F.unix_timestamp(time_col, \"yyyy-MM-dd\") - F.unix_timestamp(baseline, \"yyyy-MM-dd\")\n",
    "#     cohort = cohort.withColumn(time_col, time2eventdiff).withColumn(time_col, (F.col(time_col) / 3600 / 24 / 30).cast('integer')).drop('eventdate').drop('endfollowupdate')\n",
    "   \n",
    "#     return cohort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## acute conditions\n",
    "1. ischemic stroke\n",
    "2. acute kidney injury\n",
    "3. pulmonary embolism\n",
    "4. Abdominal aortic aneurysm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cohort_all = read_parquet(spark.sqlContext, '/home/shared/yikuan/Mo/cohort.parquet').select(['patid', 'dob', 'baseline', 'start', 'startdate', 'end', 'enddate', 'time', 'event'])\n",
    "# cohort_all = cohort_all.withColumnRenamed('event', 'mortality').withColumnRenamed('time', 'mortality_time')\n",
    "# cohort_all = cohort_all.filter(F.col('baseline')<F.col('enddate'))\n",
    "\n",
    "# # surv = SurvRiskPredictionBase(60)\n",
    "# death = tables.retrieve_death(dir=file['death'], spark=spark)\n",
    "# condition_query = MedicalDictionary.MedicalDictionaryRiskPrediction(file, spark)\n",
    "\n",
    "# # diagnosis\n",
    "# diagnoses = read_parquet(spark.sqlContext, '/home/shared/yikuan/HF_Valid/data/diagnosis.parquet')\n",
    "# diag_cprd = diagnoses.filter(F.col('source')=='CPRD').select(['patid', 'eventdate', 'medcode']).withColumnRenamed('medcode', 'code')\n",
    "# diag_hes = diagnoses.filter(F.col('source')=='HES').select(['patid', 'eventdate', 'ICD']).withColumnRenamed('ICD', 'code')\n",
    "# diagnoses = diag_cprd.union(diag_hes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # ischemic stroke\n",
    "# query = condition_query.queryDisease(['Ischaemic stroke'], merge=True)\n",
    "# code = []\n",
    "# for k,v in query['ischaemic stroke'].items():\n",
    "#     code.extend(v)\n",
    "    \n",
    "# source = process_death_diagnoses(diagnoses, death, code, 'code')\n",
    "# cohort_all = event2time(cohort_all, source, 'baseline', 'code', 60, 'ischaemic_stroke', 'ischaemic_stroke_time')\n",
    "\n",
    "# # acute kidney injury\n",
    "# query = condition_query.queryDisease(['Acute Kidney Injury'], merge=True)\n",
    "# code = []\n",
    "# for k,v in query['acute kidney injury'].items():\n",
    "#     code.extend(v)\n",
    "    \n",
    "# source = process_death_diagnoses(diagnoses, death, code, 'code')\n",
    "# cohort_all = event2time(cohort_all, source, 'baseline', 'code', 60, 'acute_kidney_injury', 'acute_kidney_injury_time')\n",
    "\n",
    "# # pulmonary embolism\n",
    "# query = condition_query.queryDisease(['Pulmonary embolism'], merge=True)\n",
    "# code = []\n",
    "# for k,v in query['pulmonary embolism'].items():\n",
    "#     code.extend(v)\n",
    "\n",
    "# source = process_death_diagnoses(diagnoses, death, code, 'code')\n",
    "# cohort_all = event2time(cohort_all, source, 'baseline', 'code', 60, 'pulmonary_embolism', 'pulmonary_embolism_time')\n",
    "\n",
    "\n",
    "# # Abdominal aortic aneurysm\n",
    "# query = condition_query.queryDisease(['Abdominal aortic aneurysm'], merge=True)\n",
    "# code = []\n",
    "# for k,v in query['abdominal aortic aneurysm'].items():\n",
    "#     code.extend(v)\n",
    "    \n",
    "# source = process_death_diagnoses(diagnoses, death, code, 'code')\n",
    "# cohort_all = event2time(cohort_all, source, 'baseline', 'code', 60, 'abdominal_aortic_aneurysm', 'abdominal_aortic_aneurysm_time')\n",
    "\n",
    "# cohort_all.write.parquet('/home/shared/yikuan/Mo/cohort_v1.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# merge outcome with berht data and imp predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop_col = ['dob', 'baseline', 'start', 'startdate', 'end', 'enddate']\n",
    "# cohort = read_parquet(spark.sqlContext, '/home/shared/yikuan/Mo/cohort_v1.parquet')\n",
    "\n",
    "# for col in drop_col:\n",
    "#     cohort = cohort.drop(col)\n",
    "\n",
    "# cohort_expert = read_parquet(spark.sqlContext, '/home/shared/yikuan/Mo/mortality_imp.parquet').drop('event').drop('time')\n",
    "# cohort_expert = cohort_expert.join(cohort, 'patid', 'left')\n",
    "\n",
    "# cohort_expert.write.parquet('/home/shared/yikuan/Mo/expert_predictor.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cohort_behrt =  read_parquet(spark.sqlContext, '/home/shared/yikuan/Mo/BEHRT.parquet').drop('event').drop('time')\n",
    "# cohort_behrt = cohort_behrt.join(cohort, 'patid', 'left')\n",
    "# cohort_behrt.write.parquet('/home/shared/yikuan/Mo/BEHRT_v1.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort = read_parquet(spark.sqlContext, '/home/shared/yikuan/Mo/mortality_imp.parquet')\n",
    "baseline = read_parquet(spark.sqlContext, '/home/shared/yikuan/Mo/cohort.parquet').select(['patid', 'baseline'])\n",
    "cohort = cohort.join(baseline, 'patid', 'left')\n",
    "\n",
    "condition_query = MedicalDictionary.MedicalDictionaryRiskPrediction(file, spark)\n",
    "\n",
    "def code_merge(code_dict, name):\n",
    "    code = []\n",
    "    for k,v in code_dict[name].items():\n",
    "        code.extend(v)\n",
    "    return code\n",
    "\n",
    "predictor = predictor_extractor.PredictorExtractorBase()\n",
    "\n",
    "# obesity\n",
    "name = 'obesity'\n",
    "code = condition_query.queryDisease([name], merge=True)\n",
    "code = code_merge(code, name)\n",
    "code = predictor.predictor_check_exist(code, diagnoses, cohort, col='code', col_baseline='baseline').withColumnRenamed('code', '_'.join(name.split()))\n",
    "cohort = cohort.join(code, 'patid', 'left')\n",
    "\n",
    "# Alcohol problems\n",
    "name = 'alcohol problems'\n",
    "code = condition_query.queryDisease([name], merge=True)\n",
    "code = code_merge(code, name)\n",
    "code = predictor.predictor_check_exist(code, diagnoses, cohort, col='code', col_baseline='baseline').withColumnRenamed('code', '_'.join(name.split()))\n",
    "cohort = cohort.join(code, 'patid', 'left')\n",
    "\n",
    "# Hypo or hyperthyroidism \n",
    "name = 'hypo or hyperthyroidism'\n",
    "code = condition_query.queryDisease([name], merge=True)\n",
    "code = code_merge(code, name)\n",
    "code = predictor.predictor_check_exist(code, diagnoses, cohort, col='code', col_baseline='baseline').withColumnRenamed('code', '_'.join(name.split()))\n",
    "cohort = cohort.join(code, 'patid', 'left')\n",
    "\n",
    "cohort.write.parquet('/home/shared/yikuan/Mo/evaluation/Predictor_evaluation_1_1.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort = read_parquet(spark.sqlContext, '/home/shared/yikuan/Mo/evaluation/Predictor_evaluation_1_1.parquet')\n",
    "\n",
    "# Valvular disease \n",
    "name = 'valvular disease'\n",
    "code = condition_query.queryDisease(['Rheumatic valve dz', 'Nonrheumatic aortic valve disorders', 'Nonrheumatic mitral valve disorders', 'Multiple valve dz'], merge=True)\n",
    "code = code_merge(code, 'merged')\n",
    "code = predictor.predictor_check_exist(code, diagnoses, cohort, col='code', col_baseline='baseline').withColumnRenamed('code', '_'.join(name.split()))\n",
    "cohort = cohort.join(code, 'patid', 'left')\n",
    "\n",
    "# Cardiomyopathy\n",
    "name = 'cardiomyopathy'\n",
    "code = condition_query.queryDisease(['Dilated cardiomyopathy', 'Hypertrophic Cardiomyopathy', 'Other Cardiomyopathy'], merge=True)\n",
    "code = code_merge(code, 'merged')\n",
    "code = predictor.predictor_check_exist(code, diagnoses, cohort, col='code', col_baseline='baseline').withColumnRenamed('code', '_'.join(name.split()))\n",
    "cohort = cohort.join(code, 'patid', 'left')\n",
    "\n",
    "# Ischaemic heart disease \n",
    "name= 'ischaemic heart disease'\n",
    "code=['14A3.00','14A4.00','14A5.00','14AA.00','14AH.00','14AJ.00','14AL.00','14AT.00','14AW.00','182A.00','187..00','1J61.00','3213111','322..00','3222','322Z.00','323..00','3232','3233','3234','3235','3236','323Z.00','32B..00','32B2.00','32B3.00','32BZ.00','32E4.00','44H3.00','44H3000','44HJ.00','44MH.00','44p2.00','5533','5543','5C11.00','661M000','662..00','662..11','662K.00','662K000','662K100','662K200','662K300','662Kz00','662N.00','662Z.00','66f..00','66f1.00','6A2..00','6A4..00','792..11','7920','7920.11','7920000','7920100','7920200','7920300','7920y00','7920z00','7921','7921.11','7921000','7921100','7921200','7921300','7921y00','7921z00','7922','7922.11','7922000','7922100','7922200','7922300','7922y00','7922z00','7923','7923.11','7923000','7923100','7923200','7923300','7923z00','7924','7924000','7924100','7924200','7924y00','7924z00','7925','7925.11','7925000','7925100','7925300','7925311','7925312','7925400','7925y00','7925z00','7926','7926000','7926200','7926300','7926z00','7927500','7928','7928.11','7928000','7928100','7928200','7928300','7928y00','7928z00','7929000','7929100','7929111','7929300','7929400','7929500','7929600','792B000','792C.00','792C000','792Cy00','792Cz00','792D.00','792Dy00','792Dz00','793G.00','793G000','793G100','793G200','793G300','793Gy00','793Gz00','7A4B800','7A54000','7A54500','7A54700','7A54800','7A56000','7A56400','7A6G100','7A6H300','7A6H400','7A6S300','889A.00','8B27.00','8B3k.00','8BGC.00','8CMP.00','8F9..00','8F90.00','8F91.00','8F92.00','8F93.00','8H2V.00','8H7v.00','8I37.00','8I3a.00','8IEY.00','8L40.00','8L41.00','8LF..00','8T04.00','9Ob..00','9Ob0.00','9Ob1.00','9Ob2.00','9Ob3.00','9Ob4.00','9Ob5.00','9Ob6.00','9Ob8.00','9Ob9.00','G....12','G....13','G3...00','G3...11','G3...12','G3...13','G30..00','G30..11','G30..12','G30..13','G30..14','G30..15','G30..16','G30..17','G300.00','G301.00','G301000','G301100','G301z00','G302.00','G303.00','G304.00','G305.00','G306.00','G307.00','G307000','G307100','G308.00','G309.00','G30A.00','G30B.00','G30X.00','G30X000','G30y.00','G30y000','G30y100','G30y200','G30yz00','G30z.00','G31..00','G310.00','G310.11','G311.00','G311.11','G311.12','G311.13','G311.14','G311000','G311011','G311100','G311200','G311300','G311400','G311500','G311z00','G312.00','G31y.00','G31y000','G31y100','G31y200','G31y300','G31yz00','G32..00','G32..11','G32..12','G33..00','G330.00','G330000','G330z00','G331.11','G332.00','G33z.00','G33z000','G33z100','G33z200','G33z300','G33z400','G33z500','G33z600','G33z700','G33zz00','G34..00','G340.00','G340.11','G340.12','G340000','G340100','G341.00','G341.11','G341000','G341100','G341z00','G342.00','G343.00','G344.00','G34y.00','G34y000','G34y100','G34yz00','G34z.00','G34z000','G35..00','G350.00','G351.00','G353.00','G35X.00','G36..00','G360.00','G361.00','G362.00','G363.00','G364.00','G365.00','G366.00','G38..00','G380.00','G381.00','G383.00','G384.00','G38z.00','G39..00','G3y..00','G3z..00','G5...00','G501.00','G574000','G5y..00','G5yyz00','G5yz.00','G5z..00','Gyu3.00','Gyu3000','Gyu3200','Gyu3300','Gyu3400','Gyu3600','Gyu5.00','Gyu7000','SP00300','SP07600','Z677.00','ZL22200','ZV45700','ZV45800','ZV45K00','ZV45K11','ZV45L00','ZV57900','I20','I200','I201','I208','I209','I21','I210','I211','I212','I213','I214','I219','I22','I220','I221','I228','I229','I23','I230','I231','I232','I233','I234','I235','I236','I238','I24','I240','I241','I248','I249','I25','I250','I251','I252','I255','I256','I258','I259','T822','Z955']\n",
    "code = predictor.predictor_check_exist(code, diagnoses, cohort, col='code', col_baseline='baseline').withColumnRenamed('code', '_'.join(name.split()))\n",
    "cohort = cohort.join(code, 'patid', 'left')\n",
    "\n",
    "cohort.write.parquet('/home/shared/yikuan/Mo/evaluation/Predictor_evaluation_1_2.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort = read_parquet(spark.sqlContext, '/home/shared/yikuan/Mo/evaluation/Predictor_evaluation_1_2.parquet')\n",
    "\n",
    "# Hypertension \n",
    "name = 'hypertension'\n",
    "code = condition_query.queryDisease([name], merge=True)\n",
    "code = code_merge(code, name)\n",
    "code = predictor.predictor_check_exist(code, diagnoses, cohort, col='code', col_baseline='baseline').withColumnRenamed('code', '_'.join(name.split()))\n",
    "cohort = cohort.join(code, 'patid', 'left')\n",
    "\n",
    "# Other interstitial pulmonary diseases with fibrosis \n",
    "name = 'other interstitial pulmonary diseases with fibrosis'\n",
    "code = condition_query.queryDisease([name], merge=True)\n",
    "code = code_merge(code, name)\n",
    "code = predictor.predictor_check_exist(code, diagnoses, cohort, col='code', col_baseline='baseline').withColumnRenamed('code', '_'.join(name.split()))\n",
    "cohort = cohort.join(code, 'patid', 'left')\n",
    "\n",
    "# Chronic obstructive pulmonary disease \n",
    "name = 'chronic obstructive pulmonary disease'\n",
    "code = ['14B3.12','H3...00','H3...11','H31..00','H310.00','H310000','H310z00','H311.00','H311000','H311100','H311z00','H312.00','H312000','H312011','H312100','H312200','H312300','H312z00','H313.00','H31y.00','H31y100','H31yz00','H31z.00','H32..00','H320.00','H320000','H320100','H320200','H320300','H320z00','H321.00','H322.00','H32y.00','H32y000','H32y100','H32y111','H32y200','H32yz00','H32z.00','H36..00','H37..00','H38..00','H39..00','H3A..00','H3y..00','H3y..11','H3y0.00','H3y1.00','H3z..00','H3z..11','H464000','H464100','H583200','Hyu3000','Hyu3100','1001','103494','104608','106650','10802','10863','10980','11150','12166','1446','14798','15157','15626','16410','21061','23492','24248','25603','26125','26306','27819','3243','33450','37247','37959','40159','40788','44525','45089','46578','56860','5710','5798','5909','59263','60188','61118','61513','63216','63479','64721','65733','66043','66058','67040','68066','68662','70787','7884','794','92955','93568','9876','99536','998','J40','J41','J42','J43','J44']\n",
    "code = predictor.predictor_check_exist(code, diagnoses, cohort, col='code', col_baseline='baseline').withColumnRenamed('code', '_'.join(name.split()))\n",
    "cohort = cohort.join(code, 'patid', 'left')\n",
    "\n",
    "cohort.write.parquet('/home/shared/yikuan/Mo/evaluation/Predictor_evaluation_1.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = ['obesity', 'alcohol problems', 'diabetes', 'hypo or hyperthyroidism',\n",
    "          'valvular disease', 'cardiomyopathy', 'ischaemic heart disease',\n",
    "          'hypertension', 'other interstitial pulmonary diseases with fibrosis', 'chronic obstructive pulmonary disease']\n",
    "\n",
    "feature = ['patid'] + ['_'.join(each.split()) for each in feature]\n",
    "\n",
    "cohort = read_parquet(spark.sqlContext, '/home/shared/yikuan/Mo/BEHRT.parquet')\n",
    "data = read_parquet(spark.sqlContext, '/home/shared/yikuan/Mo/evaluation/Predictor_evaluation_1.parquet').select(feature)\n",
    "cohort = cohort.join(data, 'patid', 'left')\n",
    "\n",
    "cohort.write.parquet('/home/shared/yikuan/Mo/evaluation/BEHRT_evaluation_1.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py3)",
   "language": "python",
   "name": "py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
